{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fad938c-d401-45c4-9cf7-45c3dcb2e92f",
   "metadata": {},
   "source": [
    "# Prompting Claude\n",
    "In this notebook we will prompt the LLM [Claude](https://www.anthropic.com/api) from [anthropic](https://www.anthropic.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ca1a6da-0241-4eb2-9e7d-905765fbafe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "464ae2b6-e44f-40b7-81ca-b00f2d9739af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_claude(message:str, model=\"claude-3-5-sonnet-20240620\"):\n",
    "    \"\"\"\n",
    "    A prompt helper function that sends a message to anthropic\n",
    "    and returns only the text response.\n",
    "\n",
    "    Example models: claude-3-5-sonnet-20240620 or claude-3-opus-20240229\n",
    "    \"\"\"\n",
    "    from anthropic import Anthropic\n",
    "    \n",
    "    # convert message in the right format if necessary\n",
    "    if isinstance(message, str):\n",
    "        message = [{\"role\": \"user\", \"content\": message}]\n",
    "        \n",
    "    # setup connection to the LLM\n",
    "    client = Anthropic()\n",
    "    \n",
    "    message = client.messages.create(\n",
    "        max_tokens=4096,\n",
    "        messages=message,\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    # extract answer\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc571c61-3cad-40e7-83ec-662fd5beb2e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_claude(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283b550a-75a5-438d-a884-60be025e855a",
   "metadata": {},
   "source": [
    "## Chat with memory\n",
    "Now we again add memory to the chat. The function shown here is identical with the function using OpenAI's API. Hence, we could combine multiple LLMs here in one chat-history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a41ffcc-dba1-475f-8ea4-7be001dbae76",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "755adbd9-17f3-428c-8145-5a6df51fd884",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt_claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bca28296-d344-4e89-a354-3990c8372d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_with_memory(message:str):\n",
    "    \"\"\"\n",
    "    This function allows to use an LLMs in a chat-mode. \n",
    "    The LLM is equipped with some memory, \n",
    "    so that we can refer back for former conversation steps.\n",
    "    \"\"\"\n",
    "    \n",
    "    # convert message in the right format and store it in memory\n",
    "    question = {\"role\": \"user\", \"content\": message}\n",
    "    chat_history.append(question)\n",
    "    \n",
    "    # receive answer\n",
    "    response = prompt(chat_history)\n",
    "    \n",
    "    # convert answer in the right format and store it in memory\n",
    "    answer = {\"role\": \"assistant\", \"content\": response}\n",
    "    chat_history.append(answer)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a548e6e-afca-4477-a381-323de349decf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Robert! It's nice to meet you. I'm an AI assistant. How can I help you today? Is there anything specific you'd like to chat about or any questions you have?\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_with_memory(\"Hi, I'm Robert.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59c15901-1977-48e6-962b-8317ad203358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Robert. You introduced yourself at the beginning of our conversation.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_with_memory(\"What's my name?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
