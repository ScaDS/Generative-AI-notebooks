{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e9c0e47-748c-44f7-a428-ab945196dafd",
   "metadata": {},
   "source": [
    "# OpenAI API\n",
    "The OpenAI Application Programming Interface (API) became a de-facto standard to communicate with LLMs programmatically. The Python interface is [open source](https://github.com/openai/openai-python).\n",
    "\n",
    "As the field is moving fast an APIs break sometimes, consider printing out the version of the library you used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82642255-0256-4296-9f7c-b931a174ad5b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.35.14'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "openai.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078253e4-5530-4c42-baa4-417ff0ff60d8",
   "metadata": {},
   "source": [
    "For accessing LLMs you create a client object first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33273854-c67c-4994-97fc-183bc6228b51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<openai.OpenAI at 0x2aaf62d7d90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = openai.OpenAI()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3646fb61-d525-4501-bb38-d5226816b7b0",
   "metadata": {},
   "source": [
    "The API expects messages in a certain format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2b589e4-bd53-4334-b1c9-869fb56af07a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': \"What's the capital of France?\"}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_messages = []\n",
    "\n",
    "my_messages.append({\n",
    "    \"role\": \"user\", \n",
    "    \"content\": \"What's the capital of France?\"\n",
    "})\n",
    "my_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf12d91-e107-4d37-9804-89ae9f9748af",
   "metadata": {},
   "source": [
    "You can send a request to the server using the `chat.completions` API. If you're planning to use ChatGPT, possible OpenAI models and their prices can be found [here](https://openai.com/api/pricing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b1086f0-812d-4c38-a4bb-8ecc7d823568",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-9mKZtUlo8l2NUTwGzF9AwXe1pcIMD', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The capital of France is Paris.', role='assistant', function_call=None, tool_calls=None))], created=1721305873, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=7, prompt_tokens=14, total_tokens=21))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\", # or: \n",
    "        messages=my_messages\n",
    "    )\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279633a8-e694-4209-9d89-934d1ebc0785",
   "metadata": {
    "tags": []
   },
   "source": [
    "The answer comes in a similar format like the request was sent. It is a list of answers actually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be100506-9450-4d82-bedd-f88621f80ca2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The capital of France is Paris.', role='assistant', function_call=None, tool_calls=None))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3234b00c-763a-4c62-817c-d62759781d19",
   "metadata": {
    "tags": []
   },
   "source": [
    "You can access the text-answer like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2bc576c-4db0-4770-82e9-d131523dc52a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9f94dc-a586-475f-9c9d-1b5496af56ee",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "For using the API, it is highly recommended to write some helper functions such as this one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70520b3a-9e68-49f8-8481-508f75c4e410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_chatgpt(message:str, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"A prompt helper function that sends a message to openAI\n",
    "    and returns only the text response.\n",
    "    \"\"\"\n",
    "    # convert message in the right format if necessary\n",
    "    if isinstance(message, str):\n",
    "        message = [{\"role\": \"user\", \"content\": message}]\n",
    "        \n",
    "    # setup connection to the LLM\n",
    "    client = openai.OpenAI()\n",
    "    \n",
    "    # submit prompt\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=message\n",
    "    )\n",
    "    \n",
    "    # extract answer\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944df69a-d98c-4c98-9038-fc37364165e8",
   "metadata": {},
   "source": [
    "This makes our life easier because we can easily access the LLM like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46f9a47b-887a-456b-9e00-11be1baed6ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are a total of six \"o\" in the word \"Woolloomoloo.\"'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_chatgpt(\"How many o are in Woolloomoloo?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
