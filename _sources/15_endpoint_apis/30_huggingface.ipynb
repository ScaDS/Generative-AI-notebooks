{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53269b57-8207-4e44-89aa-f2b108115430",
   "metadata": {},
   "source": [
    "# Huggingface API\n",
    "\n",
    "The [Huggingface](https://huggingface.co) API make open source/weight models available locally. As the model is loaded into memory, we can reuse it, and do not have to reload it entirely. This might be beneficial in scenarios where we want to prompt the same model many times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66ee90b4-389f-4c88-8688-edd3a36a8cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_hf(request, model=\"meta-llama/Meta-Llama-3.1-8B\"):\n",
    "    global prompt_hf\n",
    "    import transformers\n",
    "    import torch\n",
    "    \n",
    "    if prompt_hf._pipeline is None:    \n",
    "        prompt_hf._pipeline = transformers.pipeline(\n",
    "            \"text-generation\", model=model, model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\"\n",
    "        )\n",
    "    \n",
    "    return prompt_hf._pipeline(request)[0]['generated_text']\n",
    "prompt_hf._pipeline = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e0b317-febd-4da0-a1bc-3440b5e18855",
   "metadata": {},
   "source": [
    "We can then submit a prompt to the LLM like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "460e9067-63b9-4d11-97a0-2d97bbf5b529",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rober\\miniconda3\\envs\\genai-cpu\\Lib\\site-packages\\torch\\cuda\\__init__.py:843: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11070). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\c10\\cuda\\CUDAFunctions.cpp:108.)\n",
      "  r = torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c206a5866874d5c8a9112ab7ee9d193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device device because they were offloaded to the disk and cpu.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "C:\\Users\\rober\\miniconda3\\envs\\genai-cpu\\Lib\\site-packages\\transformers\\generation\\utils.py:1259: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'What is the capital of France? New York City\\nA. Paris\\nB. Philadelphia\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_hf(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4000bfbb-b075-4d8c-b9ce-f356383cd139",
   "metadata": {},
   "source": [
    "As the model is kept loaded in memory, a second call might be faster, not showing the model-loading output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd82b669-d8b5-454d-82bf-86ff1fc65d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'What is the capital of the Czech Republic? Prague\\n...the Czech Republic? Prague\\n...'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_hf(\"What is the capital of the Czech Republic?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa3b8f8-bb32-4d05-bd6a-3c64c26fb91d",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Explore the [HuggingFace hub for more text-generation models](https://huggingface.co/models?pipeline_tag=text-generation&sort=trending). Download one and test it using the function above. Also read its documentation and consider updating the function above according to the recommendations and examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf77db27-bff6-4268-9bdd-8f9710fa5a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
