{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afb6c4d3-37df-4f03-91ae-e2963d5940a3",
   "metadata": {},
   "source": [
    "# Fine-tuning Gemma on local hardware\n",
    "In this notebook we are fine-tuning the [google/gemma-2b-it](https://huggingface.co/google/gemma-2b-it) model using [huggingface](https://huggingface.io) infrastructure, locally on our computer using a consumer-grade graphics card with 16 GB of memory. Gemma is provided under and subject to the Gemma Terms of Use found at [ai.google.dev/gemma/terms](https://ai.google.dev/gemma/terms). This notebook was written, modifying code from [this article about fine-tuning LLama 3](https://www.datacamp.com/tutorial/llama3-fine-tuning-locally), which is highly recommeded.\n",
    "\n",
    "## Read more\n",
    "* [QLoRA](https://arxiv.org/abs/2305.14314)\n",
    "* [Gemma Cookbook](https://github.com/google-gemini/gemma-cookbook)\n",
    "* [example sft_qlora](https://huggingface.co/google/gemma-7b/blob/main/examples/example_sft_qlora.py)\n",
    "\n",
    "## Troubleshooting\n",
    "* If you run this notebook on Windows and receive error messages mentioning that CUDA initialization failed, make sure you have `bitsandbytes` version 0.43.2 or larger installed.\n",
    "* If you run out of GPU memory, make sure you use the right hardware. This notebook was developed using an RTX 3080 mobile GPU with 16 GB of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf9cefc0-98f3-4e01-bed1-df712005c086",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    ")\n",
    "import os, torch\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, setup_chat_format\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac394e1-9e00-4463-b3da-6ace85d0d2bb",
   "metadata": {},
   "source": [
    "First, we define the model we want to fine-tune and the name under which we will store the new fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1272ef91-05ea-47f9-8276-c85caea3ac9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"google/gemma-2b-it\"\n",
    "#\"google/codegemma-1.1-7b-it\"\n",
    "#\"google/gemma-2b\"\n",
    "#\"google/codegemma-2b\"\n",
    "#\"google/gemma-2b-it\"\n",
    "new_model = \"haesleinhuepf/gemma-2b-it-bia-proof-of-concept\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3315175-da23-4d9c-a96e-5a1a0015ea63",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15aedfe6-bbb7-4f4b-b883-79441f36e3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dtype = torch.float16\n",
    "attn_implementation = \"eager\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec14f12-dc8a-4ddf-876d-744a5dc3a432",
   "metadata": {},
   "source": [
    "We will use the [QLoRA](https://arxiv.org/abs/2305.14314) fine-tuning scheme, simply to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a65998bb-1d55-4a8c-b960-7c03d8a4a070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QLoRA config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa0f40d-a41c-4ef4-a8e5-e4748e7b0185",
   "metadata": {},
   "source": [
    "## Initialization of model and tokenizer\n",
    "Here we download and initialize the model and initialize the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1df3d076-ac63-4811-a5ea-2317bda1b0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb7df32cee504eae92616f203f0b5ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=attn_implementation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fac3698-01e6-4027-b424-eb4f4dcbe2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0acb3f33-8a35-4d3a-b4d8-b23db6d085bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side = 'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d52ebb5e-0fb6-44b9-b979-7528150c4290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    ")\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a0544e-c903-4230-b29c-5392412d0b66",
   "metadata": {},
   "source": [
    "## Dataset preparation\n",
    "Next we load our dataset for fine-tuning, from its [Huggingface Hub page](https://huggingface.co/datasets/haesleinhuepf/bio-image-analysis-qa). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c189d734-7a34-49b2-ad6a-bf766eeb0e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"haesleinhuepf/bio-image-analysis-qa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bde89fa-bed7-4b05-9201-b0fef89ac998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "How can we use indices in Python to crop images, similar to cropping lists and tuples?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "This code imports the necessary functions from the skimage.io module. It then reads an image called \"blobs.tif\" and assigns it to the variable 'image'. It crops the image, taking the first 128 rows, and assigns the result to 'cropped_image1'. The cropped image is then displayed using the 'imshow' function. Lastly, a list of numbers is created called 'mylist'.\n",
      "\n",
      "```python\n",
      "from skimage.io import imread, imshow, imshow\n",
      "\n",
      "image = imread(\"../../data/blobs.tif\")\n",
      "\n",
      "cropped_image1 = image[0:128]\n",
      "\n",
      "imshow(cropped_image1);\n",
      "\n",
      "mylist = [1,2,2,3,4,5,78]\n",
      "```\n",
      "<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_raw = load_dataset(dataset_name, split=\"all\")\n",
    "\n",
    "def format_chat_template(row, tokenizer):\n",
    "    row_json = [{\"role\": \"user\", \"content\": row[\"question\"]},\n",
    "               {\"role\": \"assistant\", \"content\": row[\"answer\"]}]\n",
    "    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n",
    "    return row\n",
    "\n",
    "format_chat_template_partial = partial(format_chat_template, tokenizer=tokenizer)\n",
    "\n",
    "dataset_w_template = dataset_raw.map(\n",
    "    format_chat_template_partial,\n",
    "    num_proc=4,\n",
    ")\n",
    "\n",
    "print(dataset_w_template['text'][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa1083d-e575-44d6-ba07-3e3125e6ff6f",
   "metadata": {},
   "source": [
    "We then split the data into two sets: for training and for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "085d7877-94b1-4cf3-a5fe-b79a3ec184fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'answer', 'text'],\n",
       "        num_rows: 91\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'answer', 'text'],\n",
       "        num_rows: 39\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train_test = dataset_w_template.train_test_split(test_size=0.3)\n",
    "dataset_train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64ea1d46-992e-49b4-843c-2e20afb62974",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=new_model,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=0.2,\n",
    "    logging_steps=1,\n",
    "    warmup_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    group_by_length=True,\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e43b40a6-a106-46f8-b3a7-a22ce6afcb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haase\\miniconda3\\envs\\genai-gpu\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\haase\\miniconda3\\envs\\genai-gpu\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\haase\\miniconda3\\envs\\genai-gpu\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0de43252af094f40a533d9cf89bde08c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/91 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ca57fee5b1745abb40ee54279f49772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/39 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset_train_test[\"train\"],\n",
    "    eval_dataset=dataset_train_test[\"test\"],\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=512,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing= False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb4c175d-e2bc-4963-9c1a-242fc457092e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='450' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [450/450 03:27, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.102200</td>\n",
       "      <td>1.324579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.259700</td>\n",
       "      <td>1.711752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.086800</td>\n",
       "      <td>1.999035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.064900</td>\n",
       "      <td>2.208032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.030500</td>\n",
       "      <td>2.380901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haase\\miniconda3\\envs\\genai-gpu\\Lib\\site-packages\\peft\\utils\\save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=450, training_loss=0.437066644674374, metrics={'train_runtime': 210.3913, 'train_samples_per_second': 4.325, 'train_steps_per_second': 2.139, 'total_flos': 2089069522784256.0, 'train_loss': 0.437066644674374, 'epoch': 9.89010989010989})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "903ca2a9-f921-43c8-ae3e-089d98693cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haase\\miniconda3\\envs\\genai-gpu\\Lib\\site-packages\\peft\\utils\\save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(new_model + \"_temp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1bf38f-9fc6-45d1-ba8e-13ecf592a84c",
   "metadata": {},
   "source": [
    "## Testing the model\n",
    "After the model is trained, we can do first tests with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53bf3d89-b421-4463-8578-45c80de01621",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "\n",
      "Write Python code to load the image ../11a_prompt_engineering/data/blobs.tif,\n",
      "segment the nuclei in it and\n",
      "show the result\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "The code to load the image, segment the nuclei and show the result is shown below:\n",
      "\n",
      "```python\n",
      "\n",
      "from skimage.io import imread\n",
      "from pyclesperanto_prototype import image_segmenter\n",
      "\n",
      "# load data\n",
      "blobs = imread(\"../../11a_prompt_engineering/data/blobs.tif\")\n",
      "\n",
      "# segment the nuclei\n",
      "nuclei = image_segmenter.segment_nuclei(blobs)\n",
      "\n",
      "# show the result\n",
      "cle.imshow(nuclei)\n",
      "\n",
      "```\n",
      "This code imports the `pyclesperanto_prototype` library, which contains the\n"
     ]
    }
   ],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": \"\"\"\n",
    "Write Python code to load the image ../11a_prompt_engineering/data/blobs.tif,\n",
    "segment the nuclei in it and\n",
    "show the result\n",
    "\"\"\"}]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "outputs = pipe(prompt, max_new_tokens=120, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3c58bb-4769-4d6a-81b7-d4c035ba4919",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
