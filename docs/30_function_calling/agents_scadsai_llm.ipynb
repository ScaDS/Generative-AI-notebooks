{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aecef830-39c0-4912-b959-f071ffb164b9",
   "metadata": {},
   "source": [
    "# Defining agents using ScaDS.AI's LLM service\n",
    "In this notebook we will program a simple agent using a remote institutional server and the OpenAI API. For executing this notebook, you need an environment variable named `SCADSAI_API_KEY`. You can get such a key [here](https://llm.scads.ai/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6f2d525-c3d4-457e-a029-dfa94f206b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b60f65-310f-44a4-9e34-ff4a842486cb",
   "metadata": {},
   "source": [
    "These helper functions are stored in [function_calling_scadsai.py](function_calling_scadsai.py) and not shown for the readers convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96575809-96d2-4e28-8c37-7d36bea12e6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from function_calling_scadsai import function_to_dict, act, call_function_from_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91662331-5c56-471b-94b4-b78d348b53f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tools = []\n",
    "\n",
    "@tools.append\n",
    "def generate_random_number() -> int:\n",
    "    \"\"\"Generate a random number between 0 and 100\"\"\"\n",
    "    return random.randint(0, 100)\n",
    "\n",
    "@tools.append\n",
    "def roberts_operator(a:int, b:int) -> int:\n",
    "    \"\"\"Apply Robert's operator to a and b\"\"\"\n",
    "    return a * b + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8d045cb5-fcf5-4867-ba5f-21efac174e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTINUE_PROMPT = \"\\n\\nPlease repeat your question if I should rephrase the results of the recent tool calls.\"\n",
    "TOOLCALL_PROMPT = \"\\n\\nCall tools if necessary, but don't do it if the answer is already above.\"\n",
    "CONTINUE_PHRASE = \"Let me continue to work on this\"\n",
    "\n",
    "def prompt_scadsai_llm(message, tool_dicts=[], endpoint:str= \"https://llm.scads.ai/v1\", model:str=\"meta-llama/Llama-3.3-70B-Instruct\", verbose=False, history=[]):\n",
    "    \"\"\"\n",
    "    Submit a prompt to a remotely running model and returns the response.\n",
    "    \"\"\"\n",
    "    import os\n",
    "\n",
    "    api_key = os.environ.get('SCADSAI_API_KEY')\n",
    "    \n",
    "    import openai\n",
    "    client = openai.OpenAI(base_url=endpoint,\n",
    "                       api_key=api_key)\n",
    "\n",
    "    next_message = {\"role\": \"user\", \"content\": message}\n",
    "\n",
    "    print(\"Q:\", history + [next_message])\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=history + [next_message],\n",
    "        tools=tool_dicts,\n",
    "    )\n",
    "    print(\"A:\", completion)\n",
    "        \n",
    "    return completion\n",
    "\n",
    "\n",
    "def call_function_from_response(tool_calls, named_tools, verbose=False):\n",
    "    \"\"\"\n",
    "    Takes a response-string from ollama/mistral raw mode, extracts a function to be called and its parameters and calls the function.\n",
    "    The function must be in the dictionary named_tools where keys are names of functions and values are the corresponding functions.\n",
    "    \"\"\"\n",
    "    import json\n",
    "\n",
    "    if len(tool_calls) == 0:\n",
    "        raise ValueError(\"Not a function call\")\n",
    "\n",
    "    results = []\n",
    "    for tool_call in tool_calls:\n",
    "        func = tool_call.function\n",
    "        func_name = func.name\n",
    "        arguments = json.loads(func.arguments)\n",
    "\n",
    "        result = named_tools[func_name](**arguments)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, tools, verbose=True):\n",
    "        self._tools = tools\n",
    "        self._history = []\n",
    "        self._verbose = verbose\n",
    "\n",
    "    def act(self, prompt:str, max_iterations=3):\n",
    "        \"\"\"\n",
    "        Takes a prompt and the configured list of function tools, and turns it into a prompt to a function-calling capable language model.\n",
    "        If the model can select a function to call considering the prompt, this function will be called.\n",
    "        \"\"\"\n",
    "        tools = self._tools\n",
    "        \n",
    "        # get a dictionary with names of functions as keys, and corresponding functions as values\n",
    "        named_tools = {f.__name__: f for f in tools}\n",
    "        \n",
    "        # convert the functions to json\n",
    "        tool_dicts = [function_to_dict(f) for f in tools] \n",
    "\n",
    "        if self._verbose:\n",
    "            print(\"User:\", prompt)\n",
    "        \n",
    "        # submit the prompt\n",
    "        completion = prompt_scadsai_llm(prompt + TOOLCALL_PROMPT, tool_dicts, history=self._history)\n",
    "        result = completion.choices[0].message.content\n",
    "        if self._verbose:\n",
    "            print(\"Assistant:\", result)\n",
    "\n",
    "        if completion.choices[0].message.tool_calls is not None:\n",
    "            # call the function as specified in the response\n",
    "            tool_calls = completion.choices[0].message.tool_calls\n",
    "            tool_results = call_function_from_response(tool_calls, named_tools)\n",
    "            \n",
    "            self._history.append({\"role\": \"user\", \"content\": prompt})\n",
    "            #self._history.append({\"role\": \"assistant\", \"content\": result})\n",
    "            tool_call_message = [\"I had to call some function tool(s) to answer your request:\"]\n",
    "            for tool_call, tool_result in zip(tool_calls, tool_results):\n",
    "                func = tool_call.function\n",
    "                func_name = func.name\n",
    "                arguments = func.arguments\n",
    "\n",
    "                tool_call_message.append(f\"* I called {func_name} ({arguments}) and the result was: {tool_result}\")\n",
    "\n",
    "                if self._verbose:\n",
    "                    print(\"Tool call:\", func_name, arguments, \"=\", tool_result)\n",
    "\n",
    "            response = \"\\n\".join(tool_call_message)\n",
    "            self._history.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "            if max_iterations > 0:        \n",
    "                final_phrase = f\"\\n\\nAnswer with '{CONTINUE_PHRASE}' (exactly like this) if you cannot find the answer yet.\"\n",
    "            else:\n",
    "                final_phrase = \"\"\n",
    "            continue_answer = prompt_scadsai_llm(f\"What is the ultimate answer to the following request given the discussion and tool calls we had so far?\\n\\n{prompt}{final_phrase}\",\n",
    "                                                    history=self._history,\n",
    "                                                    ).choices[0].message.content\n",
    "            \n",
    "            if max_iterations > 0 and CONTINUE_PHRASE.lower() in continue_answer.lower():\n",
    "                return self.act(prompt, max_iterations=max_iterations-1)   \n",
    "                \n",
    "            return continue_answer\n",
    "        else:\n",
    "            self._history.append({\"role\": \"user\", \"content\": prompt})\n",
    "            self._history.append({\"role\": \"assistant\", \"content\": result})\n",
    "            return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b90aae08-2165-4c76-a1c5-56a3623f483d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What is the results of Robert's operator on two random numbers?\n",
      "Q: [{'role': 'user', 'content': \"What is the results of Robert's operator on two random numbers?\\n\\nCall tools if necessary, but don't do it if the answer is already above.\"}]\n",
      "A: ChatCompletion(id='d746eeac-f7b6-485e-aa43-32696a058c38', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='0', function=Function(arguments='{\"b\": 24, \"a\": 42}', name='roberts_operator', description=None), type='function')]))], created=1734542942, model='meta-llama/Llama-3.3-70B-Instruct', object='chat.completion', service_tier=None, system_fingerprint='3.0.1-sha-bb9095a', usage=CompletionUsage(completion_tokens=25, prompt_tokens=437, total_tokens=462, completion_tokens_details=None, prompt_tokens_details=None))\n",
      "Assistant: None\n",
      "Tool call: roberts_operator {\"b\": 24, \"a\": 42} = 1009\n",
      "Q: [{'role': 'user', 'content': \"What is the results of Robert's operator on two random numbers?\"}, {'role': 'assistant', 'content': 'I had to call some function tool(s) to answer your request:\\n* I called roberts_operator ({\"b\": 24, \"a\": 42}) and the result was: 1009'}, {'role': 'user', 'content': \"What is the ultimate answer to the following request given the discussion and tool calls we had so far?\\n\\nWhat is the results of Robert's operator on two random numbers?\\n\\nAnswer with 'Let me continue to work on this' (exactly like this) if you cannot find the answer yet.\"}]\n",
      "A: ChatCompletion(id='fc0e17f6-c056-4f03-b6b7-9de6cfa26556', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"The results of Robert's operator on two random numbers is 1009\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1734542944, model='meta-llama/Llama-3.3-70B-Instruct', object='chat.completion', service_tier=None, system_fingerprint='3.0.1-sha-bb9095a', usage=CompletionUsage(completion_tokens=30, prompt_tokens=357, total_tokens=387, completion_tokens_details=None, prompt_tokens_details=None))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The results of Robert's operator on two random numbers is 1009\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Agent(tools=tools)\n",
    "a.act(\"What is the results of Robert's operator on two random numbers?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1de282be-42ec-456c-802f-6001e064dd0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What is the results of Robert's operator on 3 and 5?\n",
      "Q: [{'role': 'user', 'content': \"What is the results of Robert's operator on two random numbers?\"}, {'role': 'assistant', 'content': 'I had to call some function tool(s) to answer your request:\\n* I called roberts_operator ({\"b\": 24, \"a\": 42}) and the result was: 1009'}, {'role': 'user', 'content': \"What is the results of Robert's operator on 3 and 5?\\n\\nCall tools if necessary, but don't do it if the answer is already above.\"}]\n",
      "A: ChatCompletion(id='600b51af-9474-4c17-81f5-90ddceebf03e', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='0', function=Function(arguments='{\"b\": 5, \"a\": 3}', name='roberts_operator', description=None), type='function')]))], created=1734542946, model='meta-llama/Llama-3.3-70B-Instruct', object='chat.completion', service_tier=None, system_fingerprint='3.0.1-sha-bb9095a', usage=CompletionUsage(completion_tokens=25, prompt_tokens=503, total_tokens=528, completion_tokens_details=None, prompt_tokens_details=None))\n",
      "Assistant: None\n",
      "Tool call: roberts_operator {\"b\": 5, \"a\": 3} = 16\n",
      "Q: [{'role': 'user', 'content': \"What is the results of Robert's operator on two random numbers?\"}, {'role': 'assistant', 'content': 'I had to call some function tool(s) to answer your request:\\n* I called roberts_operator ({\"b\": 24, \"a\": 42}) and the result was: 1009'}, {'role': 'user', 'content': \"What is the results of Robert's operator on 3 and 5?\"}, {'role': 'assistant', 'content': 'I had to call some function tool(s) to answer your request:\\n* I called roberts_operator ({\"b\": 5, \"a\": 3}) and the result was: 16'}, {'role': 'user', 'content': \"What is the ultimate answer to the following request given the discussion and tool calls we had so far?\\n\\nWhat is the results of Robert's operator on 3 and 5?\\n\\nAnswer with 'Let me continue to work on this' (exactly like this) if you cannot find the answer yet.\"}]\n",
      "A: ChatCompletion(id='d23f9c45-7100-4c40-b1ac-acce5230dc05', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"The result of Robert's operator on 3 and 5 is 16\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1734542948, model='meta-llama/Llama-3.3-70B-Instruct', object='chat.completion', service_tier=None, system_fingerprint='3.0.1-sha-bb9095a', usage=CompletionUsage(completion_tokens=31, prompt_tokens=424, total_tokens=455, completion_tokens_details=None, prompt_tokens_details=None))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The result of Robert's operator on 3 and 5 is 16\""
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.act(\"What is the results of Robert's operator on 3 and 5?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32311fd2-aa52-4352-b105-bfb2d194dd58",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Defining function tools\n",
    "The following two functions will serve as functions the model can call. We also define some image memory, we can manage using the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d119103-eec1-439d-a6e7-9ab112ccdf13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_memory = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472daf09-0fe1-4fdc-9c3b-f191e8991134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an agent that uses the OpenAI GPT-4o model.\n",
    "model_client = OpenAIChatCompletionClient(\n",
    "    model=\"gpt-4o\",\n",
    "    # api_key=\"YOUR_API_KEY\",\n",
    ")\n",
    "agent = AssistantAgent(\n",
    "    name=\"fantasy_math_expert\",\n",
    "    model_client=model_client,\n",
    "    tools=[generate_random_number, roberts_operator],\n",
    "    system_message=\"Use tools to solve tasks.\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
