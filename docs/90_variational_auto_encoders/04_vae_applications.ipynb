{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applications of Variational Auto-Encoders (VAEs)\n",
    "\n",
    "In this notebook, we will explore some practical applications of Variational Auto-Encoders (VAEs). We will provide examples of using VAEs for image generation and other practical applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Generation\n",
    "\n",
    "One of the most popular applications of VAEs is image generation. VAEs can be used to generate new images that are similar to the training data. Let's see how we can use a trained VAE to generate new images."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Encoder\n",
    "\n",
    "The encoder network takes the input data and outputs the parameters of the latent distribution (mean and log variance)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2_mean = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc2_log_var = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.relu(self.fc1(x))\n",
    "        mean = self.fc2_mean(h)\n",
    "        log_var = self.fc2_log_var(h)\n",
    "        return mean, log_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Decoder\n",
    "\n",
    "The decoder network takes samples from the latent distribution and generates new data points."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, z):\n",
    "        h = self.relu(self.fc1(z))\n",
    "        x_reconstructed = self.sigmoid(self.fc2(h))\n",
    "        return x_reconstructed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the VAE\n",
    "\n",
    "The VAE model combines the encoder and decoder, and includes a sampling layer to sample from the latent distribution."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean, log_var = self.encoder(x)\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        epsilon = torch.randn_like(std)\n",
    "        z = mean + std * epsilon\n",
    "        x_reconstructed = self.decoder(z)\n",
    "        return x_reconstructed, mean, log_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Loss Function\n",
    "\n",
    "The loss function consists of the reconstruction loss and the KL divergence."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def vae_loss(x, x_reconstructed, mean, log_var):\n",
    "    reconstruction_loss = nn.functional.binary_cross_entropy(x_reconstructed, x, reduction='sum')\n",
    "    kl_divergence = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "    return reconstruction_loss + kl_divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the VAE\n",
    "\n",
    "Let's train the VAE on a simple dataset, such as the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load the MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.view(-1))])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define the VAE model\n",
    "input_dim = 28 * 28\n",
    "hidden_dim = 256\n",
    "latent_dim = 2\n",
    "encoder = Encoder(input_dim, hidden_dim, latent_dim)\n",
    "decoder = Decoder(latent_dim, hidden_dim, input_dim)\n",
    "vae = VAE(encoder, decoder)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(vae.parameters(), lr=1e-3)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "    for x, _ in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        x_reconstructed, mean, log_var = vae(x)\n",
    "        loss = vae_loss(x, x_reconstructed, mean, log_var)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch + 1}, Loss: {train_loss / len(train_loader.dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate New Images\n",
    "\n",
    "Now that we have trained the VAE, we can use it to generate new images."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Generate new data points\n",
    "z_new = torch.randn(10, latent_dim)\n",
    "with torch.no_grad():\n",
    "    generated = vae.decoder(z_new)\n",
    "\n",
    "# Plot the generated data points\n",
    "plt.figure(figsize=(10, 2))\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i + 1)\n",
    "    plt.imshow(generated[i].view(28, 28).numpy(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Detection\n",
    "\n",
    "VAEs can also be used for anomaly detection. By training a VAE on normal data, we can use the reconstruction error to detect anomalies. Data points with high reconstruction error are likely to be anomalies."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Calculate reconstruction error for test data\n",
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    reconstructed, _, _ = vae(torch.tensor(x_test, dtype=torch.float32))\n",
    "    reconstruction_error = torch.mean((torch.tensor(x_test, dtype=torch.float32) - reconstructed) ** 2, axis=1)\n",
    "\n",
    "# Plot reconstruction error\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(reconstruction_error.numpy(), bins=50)\n",
    "plt.xlabel('Reconstruction Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Reconstruction Error Histogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Compression\n",
    "\n",
    "VAEs can be used for data compression by encoding the data into a lower-dimensional latent space. The latent representation can then be used to reconstruct the original data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Encode the test data to the latent space\n",
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    mean, log_var = vae.encoder(torch.tensor(x_test, dtype=torch.float32))\n",
    "    std = torch.exp(0.5 * log_var)\n",
    "    z = mean + std * torch.randn_like(std)\n",
    "\n",
    "# Decode the latent representation to reconstruct the data\n",
    "with torch.no_grad():\n",
    "    reconstructed = vae.decoder(z)\n",
    "\n",
    "# Plot original and reconstructed data\n",
    "plt.figure(figsize=(10, 4))\n",
    "for i in range(10):\n",
    "    # Original data\n",
    "    plt.subplot(2, 10, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Reconstructed data\n",
    "    plt.subplot(2, 10, i + 11)\n",
    "    plt.imshow(reconstructed[i].view(28, 28).numpy(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ]
}
